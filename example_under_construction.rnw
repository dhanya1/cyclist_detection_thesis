% Created 2018-02-29 
% Modified 2018-04-15
\documentclass[11pt]{article}
\usepackage[a4paper,portrait,textwidth=18cm]{geometry}

\usepackage[titletoc]{appendix}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\setcounter{secnumdepth}{2}

%% INCLUDE FOLLOWING BLOCK FOR BIBLIOGRAPHY
\usepackage[
backend=biber,
style=alphabetic,
citestyle=authoryear]{biblatex}
\renewcommand*{\bibopenparen}{[}
\renewcommand*{\bibcloseparen}{]}
\renewcommand*{\finalandcomma}{,}
\renewcommand*{\finalnamedelim}{, and~}
\renewcommand*\bibnamedash{\rule[0.48ex]{3em}{0.14ex}\space}
\addbibresource{example_under_construction.bib} 
\RequirePackage{listings}
\lstset{language=Python}

\begin{document}


\begin{titlepage}
\centering
{\scshape\LARGE University College Cork\par}
\vspace{1cm}
{\scshape\Large Master's Thesis \par}
\vspace{1.5cm}
{\huge\bfseries Pedestrian Action Recognition\par}  
\vspace{0.5cm}
{\Large\bfseries Classification and tracking \par}
\vspace{2cm}
{\Large\itshape Dhanya Sringeri Jayachandra \par}
\vfill
supervised by\par
Dr. Gregory Provan
\vfill
{\large \today\par}
\end{titlepage}

\setcounter{tocdepth}{2}
\tableofcontents
\clearpage
\listoftables
\listoffigures
\clearpage

\section{Abstract}

In this paper, we will discuss and try to classify pedestrians from cyclists as accurately and quickly as possible. Accuracy and Speed are both very important aspects that could highly influence the outcome in road-safety systems. Various types of Convolutional Neural Networks approches will be applied and we can compare the advantages of one model over the other. Improving accuray of object detection and tracking are our main goal.
\clearpage

\section{Introduction}

Road-Safety is a crucial requirement in all Autonomous and Advanced Driver Assistance systems. The smart
cars are facing many challenges in Vulnerable road users recognition [ VRUs]. The objects infront of the car in traffic scene must be
detected with high accuracy to achieve fully autonomous cars. The detection of object of interest is hard in urban areas,
because of the wide variety of object appreances and occlusions caused by other objects. The similarities of object of interest with other or to the background and physical effects like cast shadows or reflections can make the distinction difficult. [cite: Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art]. We have seen examples of Computer Vision failing to recognize cyclists, and failure in recognizing pedestrians in dim light.  Though most
autonomous cars are able to classify pedestrians, it would largely help us deal with the cyclists differently.
Vulnerable road users can be detected using 2 methods.\\
(1) Sensor based approach\\
(2)Vision based approach\\

\subsection{Vision based approach:}
In vision based approach only data from the cameras are used to distinguish between objects.
Computer vision means the machine is able to get some meaningful information from an image or video. Human vision is highly
developed with the ability to understand and classify the whole scene at once, However, it was a difficult problem to gain decent 
vision results with computers up until recently, as the computers did not have the processing abilities. Though, the concept of computer vision has
been around since the 1950's, only recently we have been able to practically implement these theories. Thanks to huge volumes of 
images and videos available online. The advance in hardware with high processing power GPUs is another reason for the recent success 
which is a contributuion from the gaming industry.\\

The object detection task can be addressed with a variety of different of sensors. Cameras are the cheapest and most commonly used type of sensors for the detection of ob-
jects. The visible spectrum (VS) is typically used for daytime detections whereas the infrared spectrum can be used for night-time detection. Thermal infrared (TIR) cameras capture relative temperature which allows to distinguish warm objects like pedestrians from cold objects like vegetation or the road.[cite: Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art]


\subsection{Sensor based approach:}
In sensor based approach, the autonomous vehicles are mounted with sensors such as LiDAR systems.
The depth information of the sensor based systems result in higher accuracy in detecting an object and localizing it. These are considered supe-
rior and known to have high accuaracy in today’s research field.Depending on the weather conditions or material properties it can be problematic
to rely on a single type of sensor alone. Visible spectrum cameras and laser scanners are affected by reflective or transparent surfaces while
hot objects (like engines) or warm temperatures can influence
Thermal infrared cameras. [cite: Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art]. A combination of sensors would be the most apt method to 
generate high accuracy results.(Enzweiler, M., & Gavrila, D. M. (2011). A multilevel mixture-of-experts
framework for pedestrian classification. IEEE Trans. on Image Processing
(TIP), 20, 2967–2979.); [Chen, X., Kundu, K., Zhu, Y., Ma, H., Fidler, S., & Urtasun, R. (2016b). 3d
object proposals using stereo imagery for accurate object class detection.
arXiv.org, 1608.07711.][González, A., Vázquez, D., Lóopez, A. M., & Amores, J. (2016). On-board ob-
ject detection: Multicue, multimodal, and multiview random forest of local
experts. IEEE Trans. on Cybernetics, .]  These papers allow mention in depth about robust methods for sensor fusion. Sensors are also expensive and some car manufacturers are reluctant to compramise the look of the car for smart features.\\

In this paper, we will discuss and try to identify cyclists as accurately and quickly as possible using
vision based approach. Accuracy and Speed are both very important aspects that could highly influence the
outcome in road-safety systems. In this papers, we will compare the performance of different Convolutional
neural network architectures, their performance on the vision-based networks. We can compare the perfor-
mance of one model over the other. Improving accuray of object detection with each new network tried is
the goal.

\clearpage

\section{Literature Review}

Brief history of computer vision in Autonomous cars:\\

Around the time of 1992 - 2000 statistical machine learning models such as Support Vector Machine, Boosting started to gain popularity. The VJ detector by Viola-Jones [tobecitedcitexiaofei li new 2016] with it's detection technology was the first ground-breaking object detection algorithm, which was able to detected faces with limited amount of computational power available at the time.\\

SIFT model by David Lowe, 1999 feature based object detection. The idea is to match the features of 2 images that are constant or invariant. Based on these features the objects were classified into that particular class. This is a more efficient method than pattern matching the whole image. Spatial Pyramid matching, there are certain patterns in the image, that will hint us about what the image could be about. A varitation of this model is Histogram of Gradients[HoG], [cite: Dalal and Trings 2005] and Deformable part Model [cite: Felzenswalb, McAllester, Ramnam, 2009] performed well on Human detection. \\

In 2012, Convolutional neural networks[cite: Alex Krizhevsky, Ilya Sutskever, Geoffery E Hilton, 2012] out-performed all the other models significantly in the annual Image-net object recognition challenge, which is the bench-mark dataset for object recognition. Convolutional neural networks are performing well in classification, detection and segmentation.\\

As we are following a vision based object detection approach using using 2-D images, Convolutional neural networks are the obvious solution. They have outperformed all the previous
detection methods by significant margins. There are various variations of convolutional neural networks to choose from based on the problem at hand. We will try to work with few variations of CNNs.\\

\subsection{Detection}

The common methods followed during any traffic scene detections are,\\
1)Preprocessing the images\\
2)Extracting region of interest\\
3)Object classification\\
4)Verification\\

\subsubsection{1.Preprocessing of images}

\subsubsection{2.Extracting region of interest}

\subsubsection{3.Object classification}

\subsubsection{4.Verification}




\section{State of Art}

\subsection{Convolutional neural networks}

\textbf{Convolutional neural layer} are different from fully connected layers in the sense that they preserve the spatial structure of the image. Then we convolve the image with a filter, by sliding the filter over the image spatially and compute dot products. Filters have smaller spatial area compared to the original image, but they always have the same depth as the image. Depending on the size of the filter and the image, the size of the output always changes. \\

\[activation map =  w^T * x + b \],\\

where w = weights matrix,\\
b = bias\\

\includegraphics{Convolution}

\textbf{Convolutional neural network} consists of sequence of convolutional layers combined with activation functions, the output of each layer forms the input to the succeding layer, just like other neural networks. The network follows an hierarchial approach in terms of learning from the data. In the lower layers, the filters learn low level features like edges and progressively learns more high level features with every layer after that. This hierarchial recognition is similar to how human visual cortex works. The neurons first understand the simple concepts and then passes the information to neurons deeper level. At each higher level the neurons understand more complex features.\\

\textbf{Common Design choices}\\

\textit{Filter:} Each filter looks for a specific feature in the image. So, when we are trying to detect a certain object in the image, we use multiple filters. Each filter after convolved with the image generates an activiation map. So, the number of activation maps is equal to the number of filters used. The filter size is usually in powers of 2\\

\textit{Stride:} It gives the effect affect of downsampling the image at each layer. It also results in smaller activation maps after each layer. This would also result in smaller number of parameters to deal with.\\
Output size = (N -F)/ stride + 1\\

where, N = Image size,\\
F = Filter size,\\
stride = Number of steps taken in the next direction\\

\textit{Zero-padding:}\\
If the stride and zero-padding are of the same size, then the size of the image will remain the same. Then the formula of output size changes to,
Output size = (N+2*P - F)/ stride + 1, \\

where P = Padding size.\\

\textit{Back-propagation:}\\ [ Copied from wikipedia, needs to be modified.]
Backpropagation is a method used in artificial neural networks to calculate a gradient that is needed in the calculation of the weights to be used in the network.[1] It is commonly used to train deep neural networks,[2] a term referring to neural networks with more than one hidden layer.[3] 

The backpropagation algorithm has been repeatedly rediscovered and is equivalent to automatic differentiation in reverse accumulation mode[citation needed][clarification needed]. Backpropagation requires the derivative of the loss function with respect to the network output to be known, which typically (but not necessarily) means that a desired target value is known. For this reason it is considered to be a supervised learning method, although it is used in some unsupervised networks such as autoencoders. Backpropagation is also a generalization of the delta rule to multi-layered feedforward networks, made possible by using the chain rule to iteratively compute gradients for each layer. It is closely related to the Gauss–Newton algorithm, and is part of continuing research in neural backpropagation. Backpropagation can be used with any gradient-based optimizer, such as L-BFGS or truncated Newton[citation needed][clarification needed]. 

Mini-batch Stochastic Gradient descent:

Loop:
1. Sample a batch of data\\
2. Forward propagatation it through the graph, get loss\\
3. Backpropagation to calculate gradients\\
4. Update the parameters using the gradient\\



\textbf{Other layers in Convolutional neural network}\\
\textit{Pooling layer:}\\
It takes the activation map from the previous layer and spatially downsamples it, hence making the representation easier. The input depth is going to be the same as output depth.
Most popular apprach of pooling is Max pooling. Max pooling partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum.\\

Some design choices with pooling layers are:\\
1) The spatial extent ie F or filter size\\
2) The stride S \\

If the dimension of input is W1 * H1 * D1, then the output size of pooling layer W2 * H2 * D2 can be calculated as:\\
W2 = (W1 - F)/S + 1\\
H2 = (H1 - F)/S + 1\\
D2 = D1\\

The function of this layer is to reduce the number of computation parameters, hence it does not introduce any new parameters. \\

\textit{Fully connected layer:}\\
This is usually used at the end of convolutional neural networks. Here, we aggregate all the features detected and then use it to classify what has been detected. We can also get the score out of it.

\subsubsection{CNN Architectures}

\textbf{AlexNet:} \\
AlexNet[cite: AlexNet author] was the winner of Imagenet challenge in 2012. It outperformed all the existing methods for image classification and was the first network to put convolutional neural network on the map. From here on CNN's gained huge popularity in the field on Computer vision. It has 5 convolutional layers and 2 fully connected layers before the final classification of output classes. They have ReLU as activation 


\textbf{VGG:}\\

\textbf{GoogleNet:}\\

\textbf{ResNet:}\\






Computer vision approach in detecting vulnerable road users 

Brief:\\
Convolutional neural networks were introduced in 2014, ever since then they have been able to produce phenomenol results in the field of computer vision.
These are often the most sought after method while trying to solve any object detection or localization tasks. 





 ever since then they have bet all the previous meth-
ods in terms of object detection. In 2014, R-CNN [Regional Convolutional neural network] won the imagenet
challenge by a significant margin compared to previous methods. It used a region proposal method on the
image, then each proposed region was fed to seperate CNN and finally the object was detected.
Fast-RCNN introduced using the regional proposal method on the feature map that was extractred by
the CNN, hence saving a lot of computation time. Faster-RCNN as we have used in this model, uses it’s
own region proposal method based on the values computed by during the feature map generator instead of
using an external regional proposal algorithm. This makes the system 100 times faster than RCNN.
Faster RCNN consists of 2 parts:
(1)Feature Extractor\\
(2)Region Proposal network\\
\subsubsection{Feature Extractor}:
The feature extractor localises the parts of the image based on features. Some of the popular feature
extractors are VGGnet, Resnet and Inception. So far, Resnet is known to have the highest accuracy. It is
a very deep model with 101 hidden layers. Due to gradient explosion problem, it was impossible to have
such deep layers up until the invention of Resnet. Resnet won the imagenet object detection challenge in
2015, beating other competitors in the field by significant margin. We will be using Resnet as our feature
extractor in our model.\\
\subsection{Region Proposal network}:
Another CNN model that is considered in this setting is,
Faster Region based convolutional neural networks:\\
\section{Datasets}

The dataset that is considered in this experiment is available for public as tsinga-dailmer dataset/

\section{Experiment}:
I have divided the dataset of cyclists in 3 different views. Narrow, Intermediate and wide based on the
aspect ratio of the bounding box of cyclists. It’s important to train the network on the different views of
cyclists seperately as the cyclists look completely different from different views.

\subsection{Training:}

\subsubsection{Framework:}
\subsection{Parameter optimization}
Stride size: Reduced the stride size from 16 to 8, even though the computation time is longer, more
information needs to processed.\\

Further reduced the stride size of feature extractor from 8 to 4, to not miss out on any information.

\subsection{Region Proposal network}
Region proposal networks generate proposals from the image fed to the network which might contain
the object. So, RPN is responsible for the network to decide if an object is in foreground or background.
In the first stage anchor generator, I am reducing the stride, so that more regions are proposed and none of
the small cyclists are missed out.
I have 4 anchors of scales 0.25, 0.5, 0.75 and 1.0. Each anchor will have 3 boxes with aspect ratio of
1:1(Intermidiate), 1:2(Narrow view), 2:1(Wide view).
The maximum number of box proposals from the first stage was changed to 300. Used drop-out to
avoid overfitting of the neurons and improve accuracy. Drop out increases the number of possible iterations
required to converge. Hence, we increase the number of iterations.\\

\subsection{Regularization:}
Regularization modifies the objective function that we minimize by adding additional terms that penalize
large weights. In other words, we change the objective function so that it becomes Error+f(), where f()
grows larger as the components of grow larger and is the regularization strength (a hyper-parameter for
the learning algorithm).\\

\subsection{L2 Regularization}
It can be implemented by augmenting the error function with the squared magnitude of all weights in
the neural network. In other words, for every weight w in the neural network, we add 1/2 w**2 to the
error function. The L2 regularization has the intuitive interpretation of heavily penalizing ”peaky” weight
vectors and preferring diffuse weight vectors. This has the appealing property of encouraging the network
to use all of its inputs a little rather than using only some of its inputs a lot. Of particular note is that dur-
ing the gradient descent update, using the L2 regularization ultimately means that every weight is decayed
linearly to zero. Because of this phenomenon, L2 regularization is also commonly referred to as weight decay.\\

\subsection{Hard negative mining}

\subsection{Hyper-parameter tuning}

\subsubsection{R-FCN network:}
RFCN pretrained on Coco image dataset is used. This is a much bigger imageset with 1.5 million images. Using this model
the training period reduced drastically.
This network is used with blah blah
\section{Accuracy}

\begin{center}
 \begin{tabular}{||c c c c||} 
 \hline
 Model & Modifications & Narrow & Intermediate & Wide \\ [0.5ex] 
 \hline
FasterRCNN +Resnet & No finetuning & 53.2 & 22 & 787 \\ 
 \hline
FasterRCNN +Resnet & Increased region proposals & 55 & 33 & 5415 \\
 \hline
FasterRCNN + Resnet & dropout & 59 & 32 & -\\
 \hline
 R-FCN + Resnet & No finetuning & 50 & 33 & -\\
 \hline

 \hline
\end{tabular}
\end{center}




\section{Future work:}

We could calculate the distance between car and cyclists to determine if any action must be taken. We could also conduct the same experiments mentioned above in dim light and night time environments to understand how well the models would perform under challenging circumstances.

\printbibliography

\end{document}