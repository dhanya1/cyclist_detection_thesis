% Created 2018-02-29 
% Modified 2018-04-15
\documentclass[12pt]{article}
\usepackage[a4paper,portrait,textwidth=18cm]{geometry}

\usepackage[titletoc]{appendix}
\usepackage{graphicx}
\graphicspath{ {./images/} }

%% INCLUDE FOLLOWING BLOCK FOR BIBLIOGRAPHY
\usepackage[backend=bibtex]{biblatex}
\bibliographystyle{plain}
\bibliography{example_under_construction} 
\RequirePackage{listings}
\lstset{language=Python}

\begin{document}


\begin{titlepage}
\centering
{\scshape\LARGE University College Cork\par}
\vspace{1cm}
{\scshape\Large Master's Thesis \par}
\vspace{1.5cm}
{\huge\bfseries Cyclist Detection for Autonomous Cars\par}  
\vspace{0.5cm}
{\Large\bfseries Detection and Classification \par}
\vspace{2cm}
{\Large\itshape Dhanya Sringeri Jayachandra \par}
\vfill
supervised by\par
Dr. Gregory Provan
\vfill
{\large \today\par}
\end{titlepage}



\hfil {\LARGE \textbf{Abstract}}\hfil

\vspace{1.5cm}
{\Large In this paper, we will discuss and try to classify the different views of cyclists with respect to the position of the car using various deep learning algorithms. Cyclist detection is considered as a challenging problem in the field of object detection owing to the variances in the view of cyclists, can make the object look completely different from one another. Little work has been dedicated towards identifying cyclists in the past. Most of the self-driving cars focus on pedestrian detection. But, the behaviour of cyclists and pedestrians are different and cyclists have to be identified and handled differently. There are reported incidences of cyclists getting into an accident with highly reliable and efficient autonomous systems. This is only stressing on the point that more attention is to be given to cyclists. Categorizing the cyclists into different views would help the car understand the situation better and take appropriate decisions. Suppose, if a cyclist is crossing the road in front of the car, the only option for the car is to slow down or stop. But, if a cyclist is in just riding in front of the car, this view is completely different and the car just have to treat the cyclist like any other object. These 2 actions looks completely different from a computer vision point of view, so identifying them and classifying them in real - world scenario is what we are trying to achieve in this thesis. Accuracy and Speed are both very important aspects that could highly influence the outcome in road-safety systems. While, object detection from a fixed point camera might be more easier to interpret, the objective of this paper is to develop a algorithm that can be used by moving car, so the algorithm has to generalize well for all backgrounds and lighting conditions. Various types of Convolutional Neural Networks approaches will be applied and we can compare the advantages of one model over the other. Improving accuracy of object detection and classifying are our main goal.\par}
\clearpage

\hfil {\LARGE \textbf{Acknowledgements}}\hfil

\vspace{1.5cm}
{\Large I would like to express my profound gratitude to my academic advisor, Dr. Gregory Provan, for his scholastic advice and technical guidance throughout this research. His devotion, patience, and focus on excellence allowed me to reach this important milestone in my life. I also wish to extend my acknowledgements to the company Valeo labs, Tuem especially to Dr.Ciaran Hughes and Jonathan Horgan for their continued support, feedback, guidance and suggestions.
I wish to thank University College Cork for providing unlimited assistance and support during my study. \par}

\clearpage

\hfil {\LARGE \textbf{Dedication}}\hfil

\vspace{1.5cm}
{\Large This study is dedicated to my husband, Hithyshi Krishnamurthy, and my parents, Jayachandra and Shakila, also my in-laws Vinoda and Krishnamurthy. Without their continued and unconditional love and support, I would not be the person I am today.\par}
\vfill
\clearpage

\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\tableofcontents
\clearpage
\listoftables
\listoffigures
\clearpage


\section{Introduction}

The major challenge and crucial challenge in the complete autonomy of the cars is road-safety. Though there are pedestrian and cyclist detection methods available in every
Autonomous and Advanced Driver Assistance systems[ADAS], they still produce high rate of errors in real world scenes. The smart
cars are facing many challenges in Vulnerable road users recognition [ VRUs]. The objects in front of the car in traffic scene must be
detected with high accuracy to achieve fully autonomous cars. We have seen examples of Computer Vision failing to recognize cyclists, and failure in recognizing pedestrians in dim light. On KITTI dataset challenge leaderboard, we can see networks performing highly efficiently in detecting Cars and Pedestrians but giving very poor results in case of Cyclist detection and recognition.

Though most
autonomous cars are able to classify pedestrians, it would largely help us deal with the cyclists who are approaching the car in different directions differently. Identifying and classifying cyclists into different views will give the ADAS more information on how to deal with the issue next.
Vulnerable road users can be detected using 2 methods.\\
(1) Sensor based approach\\
(2)Vision based approach\\

\subsection{Vision based approach:}

  In vision based approach only data from the visual sensors (cameras) are used to classify and localize objects.
Computer vision techniques continually strive to get some meaningful information from an image or video. Human vision is highly developed with the ability to understand and classify the any object or even the entire surrounding scene at once. However, it is a difficult problem to gain decent 
vision results with computers up until recently. Though, the concept of computer vision has
been around since the 1950's, only recently we have been able to practically implement these theories. The major challenges in the field of computer vision were there was not enough data available to train the models. The Computers did not have the computing ability to execute the theoretical solutions. 
Thanks to huge volumes of 
images and videos available on the internet and advance in hardware with Graphical Processing Units is another reason for the recent success which is a contribution from the gaming industry. Computer vision has become a major contributor to all the cutting edge applications including Autonomous vehicles.

Today, there are still many issues with Computer-Vision based algorithms. The detection of object of interest is a difficult problem in urban areas,
because of the wide variety of object appearances and occlusions caused by other objects. The similarities of cyclists with other or to the background and physical effects like shadows, illumination and reflections can make the distinction difficult. \cite{janai_computer_2017}. The wide range of bikes of different styles and pedestrians, change in color, size of the object poses an added difficulty.  

 Cameras are the cheapest and most commonly used type of sensors for the detection of objects. The visible spectrum (VS) is typically used for detection during daytime whereas the infrared spectrum can be used for night-time detection. Thermal infrared (TIR) cameras capture relative temperature which allows to distinguish warm objects like pedestrians from cold objects like vegetation or the road.\cite{janai_computer_2017}


\subsection{Sensor based approach:}

  In sensor based approach, the autonomous vehicles are mounted with stereo cameras, LiDAR systems and  millimeter-wave radar.
  The object detection task can be addressed with a variety of different of sensors.
The depth information of the sensor based systems result in higher accuracy in detecting an object and localizing it. These are considered superior and known to have high accuracy in today’s research field.Depending on the weather conditions or material properties it can be problematic
to rely on a single type of sensor alone. Visible spectrum cameras and laser scanners are affected by reflective or transparent surfaces while
hot objects (like engines) or warm temperatures can influence
Thermal infrared cameras. \cite{janai_computer_2017}. It is better to use a combination of sensors to generate high accuracy results\cite{enzweiler_multilevel_2011}. Sensors are also expensive and some car manufacturers are reluctant to compromise the look of the car for smart features.
In this paper, we will discuss and try to identify cyclists as accurately and quickly as possible using
vision based approach. Accuracy and Speed are both very important aspects that could highly influence the
outcome in road-safety systems. In this papers, we will compare the performance of different Convolutional neural network architectures, their performance on the vision-based networks. We can compare the performance of one model over the other. Improving accuracy of object detection with each new network tried is
the goal.
\clearpage

\section{Literature Review}
\subsection{Related Work:}

\subsubsection{Object Detection:}
Around the time of 1992 - 2000 statistical machine learning models such as Support Vector Machine, Boosting started to gain popularity. The VJ detector by Viola-Jones \cite{viola_detecting_2003} with it's detection technology was the first ground-breaking object detection algorithm, which was able to detected faces with limited amount of computational power available at the time. SIFT model by David Lowe, 1999 feature based object detection. The idea is to match the features of 2 images that are constant or invariant. Based on these features the objects were classified into that particular class. This is a more efficient method than pattern matching the whole image. Another method, Spatial Pyramid matching is based on finding certain patterns in the image, that will hint us about what the image could be about. A variation of this model is Histogram of Gradients[HoG], \cite{dalal_histograms_2005} and Deformable part Model \cite{felzenszwalb_object_2010}  performed well on Human detection. There are certain problems where the traditional approaches such as this are still continually outperforming all the other methods.

\subsubsection{Computer Vision with Deep Learning}


\subsubsection{Convolutional neural layer} are different from fully connected layers in the sense that they preserve the spatial structure of the image. Then we convolve the image with a filter, by sliding the filter over the image spatially and compute dot products. Filters have smaller spatial area compared to the original image, but they always have the same depth as the image. Depending on the size of the filter and the image, the size of the output always changes. 

\[activation map =  w^T * x + b \] 
where,\\
w = weights matrix\\
b = bias

\subsubsection{Convolutional neural network} 
Convolutional neural network consists of sequence of convolutional layers combined with activation functions, the output of each layer forms the input to the succeeding layer, just like other neural networks. The network follows an hierarchical approach in terms of learning from the data. In the lower layers, the filters learn low level features like edges and progressively learns more high level features with every layer after that. This hierarchical recognition is similar to how human visual cortex works. The neurons first understand the simple concepts and then passes the information to neurons deeper level. At each higher level the neurons understand more complex features.

Computer vision approach in detecting vulnerable road users 

Brief:\\
Convolutional neural networks were introduced in 2014, ever since then they have been able to produce phenomenal results in the field of computer vision.
These are often the most sought after method while trying to solve any object detection or localization tasks. 
ever since then they have bet all the previous methods in terms of object detection.

\subsubsection{Common Design choices}

\textit{Filter:} Each filter looks for a specific feature in the image. So, when we are trying to detect a certain object in the image, we use multiple filters. Each filter after convolved with the image generates an activation map. So, the number of activation maps is equal to the number of filters used. The filter size is usually in powers of 2

\textit{Stride:} It gives the effect affect of down sampling the image at each layer. It also results in smaller activation maps after each layer. This would also result in smaller number of parameters to deal with.\\
Output size = (N -F)/ stride + 1

where, N = Image size,\\
F = Filter size,\\
stride = Number of steps taken in the next direction

\textit{Zero-padding:}\\
If the stride and zero-padding are of the same size, then the size of the image will remain the same. Then the formula of output size changes to,
Output size = (N+2*P - F)/ stride + 1, 

where P = Padding size.

\textit{Number of Neurons:}\\
The product of the output height and width gives the total number of neurons in a feature map, say Map Size. The total number of neurons (output size) in a convolutional layer, then, is Map Size*Number of Filters.

\textit{Output Size:}\\
The output height and width of a convolutional layer is (Input Size – Filter Size + 2*Padding)/Stride + 1. This value must be an integer for the whole image to be fully covered. If the combination of these parameters does not lead the image to be fully covered, the software by default ignores the remaining part of the image along the right and bottom edge in the convolution.

\textit{Back-propagation:}\\  
Copied from wikipedia, needs to be modified.
Backpropagation is a method used in artificial neural networks to calculate a gradient that is needed in the calculation of the weights to be used in the network.[1] It is commonly used to train deep neural networks,[2] a term referring to neural networks with more than one hidden layer.[3] 

The backpropagation algorithm has been repeatedly rediscovered and is equivalent to automatic differentiation in reverse accumulation mode[citation needed][clarification needed]. Back propagation requires the derivative of the loss function with respect to the network output to be known, which typically (but not necessarily) means that a desired target value is known. For this reason it is considered to be a supervised learning method, although it is used in some unsupervised networks such as autoencoders. Back propagation is also a generalization of the delta rule to multi-layered feedforward networks, made possible by using the chain rule to iteratively compute gradients for each layer. It is closely related to the Gauss–Newton algorithm, and is part of continuing research in neural backpropagation. Backpropagation can be used with any gradient-based optimizer, such as L-BFGS or truncated Newton[citation needed][clarification needed]. 

Mini-batch Stochastic Gradient descent:

Loop:
1. Sample a batch of data\\
2. Forward propagation it through the graph, get loss\\
3. Back propagation to calculate gradients\\
4. Update the parameters using the gradient

\subsubsection{Other layers in Convolutional neural network}

\textit{Input layer:}\\
The image input layer defines the size of the input images of a convolutional neural network and contains the raw pixel values of the images. The size of an image corresponds to the height, width, and the number of color channels of that image. This layer can also perform data normalization by subtracting the mean image of the training set from every input image.

\textit{Pooling layer:}\\
It takes the activation map from the previous layer and spatially downsamples it, hence making the representation easier. The input depth is going to be the same as output depth.
Most popular approach of pooling is Max pooling. Max pooling partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum.

Some design choices with pooling layers are:\\
1) The spatial extent ie F or filter size\\
2) The stride S 

If the dimension of input is W1 * H1 * D1, then the output size of pooling layer W2 * H2 * D2 can be calculated as:\\
W2 = (W1 - F)/S + 1\\
H2 = (H1 - F)/S + 1\\
D2 = D1

The function of this layer is to reduce the number of computation parameters, hence it does not introduce any new parameters. 

\textit{Fully connected layer:}\\
This is usually used at the end of convolutional neural networks. All the neurons of the present layer connects to all the neurons of the previous layer. Here, we aggregate all the features detected and then use it to classify what has been detected. We can also get the score out of it. These layers usually have huge number of parameters to calculate.

\textit{Normalization layer:}

** Copied**
The normalization layers are introduced between convolutional layers and pooling layers to normalize the irregularities and speed up network training and reduce the sensitivity to network initialization. The layer first normalizes the activation of each channel by subtracting the mini-batch mean and dividing by the mini-batch standard deviation. Then, the layer shifts the input by an offset β and scales it by a scale factor γ. β and γ are themselves learnable parameters that are updated during network training. Create a batch normalization layer using batch Normalization Layer.

\textit{Dropout Layer:}

A dropout layer randomly sets the layer’s input elements to zero with a given probability. Create a dropout layer using the dropout Layer function.

Although the output of a dropout layer is equal to its input, this operation corresponds to temporarily dropping a randomly chosen unit and all of its connections from the network during training. So, for each new input element, train Network randomly selects a subset of neurons, forming a different layer architecture. These architectures use common weights, but because the learning does not depend on specific neurons and connections, the dropout layer might help prevent overfitting [7], [2]. Similar to max- or average-pooling layers, no learning takes place in this layer.

\subsubsection{CNN Architectures}

\textbf{AlexNet:} \\
AlexNet[cite: AlexNet author] was the winner of Imagenet challenge in 2012. It outperformed all the existing methods for image classification and was the first network to put convolutional neural network on the map. From here on CNN's gained huge popularity in the field on Computer vision. It has 5 convolutional layers and 2 fully connected layers before the final classification of output classes. They have ReLU as activation function. \\
\textbf{ZF-net:}\\
In 2013, ZF-net[cite: ZFNet author]  won the imagenet challenge. Similar to Alex-net with different hyper-parameters. ZF-Net had 8 layers\\
\textbf{VGG:}\\
In 2013, runnerup of imagenet classification challenge, VGG-Net was the best network for solving localization challenge. It had 2 variations with 16 layer and 18 layers deep model.They kept very small filters and periodic pooling.\\ 
\textbf{GoogLeNet:}\\
In 2013, winner of imagenet classification challenge was GoogleNet. It was 22 layers deep model.They introduced the new concept called inception modules. \\
\textbf{ResNet:}\\
This is the current best architecture in the field. Resnet is 152 layers deep. It won almost all the challenges in the year 2015. Proving that deep networks are clearly better than anything else. This is the chosen architecture for training in our experiment
ResNet stands for residual networks, here we use networks to fit a residual mapping instead of directly trying to fit a desired underlying mapping. \\

\subsection{Object detection Algorithms:}

\subsubsection{Regional - Convolutional neural networks:}
To bypass the problem of selecting a huge number of regions, Ross Girshick et al. proposed a method where we use selective search to extract just 2000 regions from the image and he called them region proposals. Therefore, now, instead of trying to classify a huge number of regions, you can just work with 2000 regions. These 2000 region proposals are generated using the selective search algorithm which is written below.

Problems with R-CNN

It still takes a huge amount of time to train the network as you would have to classify 2000 region proposals per image.
It cannot be implemented real time as it takes around 47 seconds for each test image.
The selective search algorithm is a fixed algorithm. Therefore, no learning is happening at that stage. This could lead to the generation of bad candidate region proposals.

\subsubsection{Fast - RCNN:}

The same author of the previous paper(R-CNN) solved some of the drawbacks of R-CNN to build a faster object detection algorithm and it was called Fast R-CNN. The approach is similar to the R-CNN algorithm. But, instead of feeding the region proposals to the CNN, we feed the input image to the CNN to generate a convolutional feature map. From the convolutional feature map, we identify the region of proposals and warp them into squares and by using a RoI pooling layer we reshape them into a fixed size so that it can be fed into a fully connected layer. From the RoI feature vector, we use a softmax layer to predict the class of the proposed region and also the offset values for the bounding box.

The reason “Fast R-CNN” is faster than R-CNN is because you don’t have to feed 2000 region proposals to the convolutional neural network every time. Instead, the convolution operation is done only once per image and a feature map is generated from it.

From the above graphs, you can infer that Fast R-CNN is significantly faster in training and testing sessions over R-CNN. When you look at the performance of Fast R-CNN during testing time, including region proposals slows down the algorithm significantly when compared to not using region proposals. Therefore, region proposals become bottlenecks in Fast R-CNN algorithm affecting its performance.

\subsubsection{Faster - RCNN:}

Both of the above algorithms(R-CNN \& Fast R-CNN) uses selective search to find out the region proposals. Selective search is a slow and time-consuming process affecting the performance of the network. Therefore, Shaoqing Ren et al. came up with an object detection algorithm that eliminates the selective search algorithm and lets the network learn the region proposals.

Similar to Fast R-CNN, the image is provided as an input to a convolutional network which provides a convolutional feature map. Instead of using selective search algorithm on the feature map to identify the region proposals, a separate network is used to predict the region proposals. The predicted region proposals are then reshaped using a RoI pooling layer which is then used to classify the image within the proposed region and predict the offset values for the bounding boxes.

From the above graph, you can see that Faster R-CNN is much faster than it’s predecessors. Therefore, it can even be used for real-time object detection.

arly to zero. Because of this phenomenon, L2 regularization is also commonly referred to as weight decay.

\subsection{Hard negative mining}
n order to train a detector with TFODAPI you don’t need to provide negative examples: the trainer can use your labeled images as well for this purpose. The procedure is called online hard-negative mining and that is why: for each “negative” generated box (that is: such box that its overlap with any of the ground truth boxes is less than specified threshold) on feature map the classification loss is calculated (it has to be high as far as it is not labeled as any object instance), then these negative boxes are sorted by their respective loss in descending order and top-k of them are used for back propagation. Three questions arise here.

Well, in case of object detection, you most probably have not-so-many objects on training images (usually less than a dozen), so the largest area on the image is a background (otherwise it wouldn’t be called background, right?). So taking all of the negatives leads to a great imbalance of the dataset towards negative examples, and that is of no good for a good training. In order to keep your dataset balanced you can fix the number of negative examples per single positive one — in this case your dataset will always have a fixed ratio of class representatives (for example, 3/1).

Imagine that you take random negatives. Random here means that we don’t take into account how hard for our model these examples are — and, probably, we sometimes will force our model to train on examples that are already chewed. Now imagine that you take only the hard ones. That’s it.

\subsection{Hyper-parameter tuning}

\subsubsection{R-FCN network:}
RFCN pretrained on Coco image dataset is used. This is a much bigger imageset with 1.5 million images. Using this model
the training period reduced drastically.
This network is used with blah blah

In 2012, Convolutional neural networks\cite{krizhevsky_imagenet_2012} out-performed all the other models significantly in the annual Image-net object recognition challenge. Image-Net is one of the bench-mark dataset for object recognition.Image-Net competition is where all the algorithms developed in that field are tested against the same data set and their error-rate is compared. Convolutional neural networks are performing well in classification, detection and segmentation. As we are following a vision based object detection approach using using 2-D images, Convolutional neural networks are the obvious solution. They have outperformed all the previous detection methods by significant margins. There are various variations of Convolutional Neural Networks to choose from based on the problem at hand. We will try to work with few variations of CNNs.



\subsubsection{Cyclist Detection:}
There are very few papers that have focused on the problem of cyclist detection. The most recent ones, also the only papers to have used Deep learning techniques to solve the cyclist detection papers are discussed below. In 2016, Xiaofei Li[cite:] and team presented a paper that was entirely dedicated to solving the Cyclist detection problem. Till then, most of the traffic scene detection datasets and Pedestrian detection datasets completely left out cyclists or had very few instances of Cyclists. This paper introduced a benchmark dataset for Cyclist detection, which is collected by driving around in the Beijing city, China. The dataset represents has close-to real world scenes, with highly occluded Cyclists, cyclists that are similar to other riders, cyclists that are similar to backgrounds.

The cyclists are divided into 3 different views based on the aspect ratio of bikes to deal with the variation of view in cyclists. The paper majorly focuses on detecting and identifying a object as cyclist. The information about the orientation of cyclist with respect to the car is not the output. The paper takes 3 different approaches to detect cyclists on the dataset they have created. Two traditional approaches such as Aggregated Channel Features[cite:], Deformable Part Model[cite:] and Region Based convolutional neural networks. Since, in this paper we are interested in detecting the cyclists with Convolutional neural network approach, check more details about how this model was built. Region based convolutional neural networks[RCNNs] are driving the success in the field of object detection. These are explained in detail below. This paper introduced a special architecture of their own called Stereo-Proposal based Fast R-CNN, which derives it's Region proposal methods from Stixel world\cite{enzweiler_efficient_2012}. Every object detector uses a region proposal method, to identify potential regions. The images are first converted to Stixels and the region is proposed from this new picture. The proposed region is observed on the original image to classification and localization part of the network, where bounding box regressors generate a bounding box for the object and classifies it into a specific class. The stixel generation is done based on the paper,  \cite{enzweiler_efficient_2012}. The open source of ZF-nets and VGG-nets are employed in this work of Fast-RCNN architecture. The image side was restricted to 650 pixels and 1300 pixels, due to limited Graphics memory. In this experiment they found that Convolutional neural networks have not performed as good as the other traditional counterparts. Though, the much Faster counterpart of Faster-RCNN was available during this period, the team has used Fast- RCNN framework. This is probably due to the fact that they wanted to implement external Region Proposal Network and not the automatic Region proposal network, that is inbuilt in Faster RCNN.

In 2017, Khaled Shah \cite{saleh_cyclist_2017} and team proposed a paper dedicated to Cyclist detection using Faster-RCNN and KITTI\cite{geiger_vision_2013} dataset using LiDar data. In this method, the authors have used depth data of the cyclist to propose regions that potentially contain a cyclist. They synthetically generated 10K annotated depth image of cyclists. The paper has then compared 3 different networks Faster RCNN with changing the shared ConvNet between different networks. The network with Faster RCNN and VGGNet16 has given the best result. With the depth image, they were able to get good results with accuracy at rate 89\%, whereas the Deep learning method mentioned above \cite{xiaofei_li_new_2016} achieved only 75\% accuracy. The training data used here was generated synthetically, but it was able to generalize well on LiDar test data. Expect for minimizing the loss function using mini-Stochastic Gradient Descent no other fine tuning was applied to the original faster RCNN network. Unlike, the Vision based approach, in this paper with Depth information the Convolutional Neural Networks have outperformed the traditional approaches.

\section{Dataset}

Since pre-trained models are easier to train and converge faster than training models from scratch, I have considered models that were  pretrained on KITTI\cite{geigerAreWeReady2012} and Coco dataset. The dataset that was used for training was Tsinghua-Daimler benchmark dataset introduced by Xiaofei Li \cite{xiaofei_li_new_2016}.

\subsection{KITTI Dataset}
There is plenty of data available for pedestrian detection like CALTECH benchmark dataset. KITTI \cite{geigerAreWeReady2012} dataset is the benchmark traffic scene recognition. It has about 2000 cyclists. But, the ratio of number of cyclists available is less compared to the number of cars or pedestrians. Most the algorithms competing in KITTI \cite{geigerAreWeReady2012} object recognition challenge in the KITTI\cite{geigerAreWeReady2012} leader board perform very well with Car and Pedestrians but perform poorly on Cyclists. Maybe due to the lack of number of cyclists used during training. The KITTI\cite{geigerAreWeReady2012} dataset has recorded the traffic scene by driving the car around in a small city, rural areas and highways. This is possibly the most challenging and close to real world traffic scene detection dataset. Since, the data I will be using for training is also collected by driving the car around the city, the view of the images would be similar. Hence, I chose the model pre-trained on KITTI dataset.

\subsection{COCO Dataset}
The COCO dataset with 2.5 million labeled instances in 328k images is a benchmark dataset in the field of object detection. The networks pretrained on COCO dataset are said to perform fairly well on generic object detection problems. The dataset has 91 different classes including significant number of cyclists, cycles, motorcyclists and pedestrians. I have also used a model that is pre-trained on Coco Dataset. But, soon I found that the model pre-trained on KITTI dataset converges faster than Coco Dataset and hence, continued with fine-tuning the model that was pre-trained on KITTI dataset.

\subsection{Tsinghua-Daimler Cyclist Benchmark Dataset}

Till 2016, there was no large-scale dataset containing large variation of cyclists and that could attribute to the large variety of variations in the cyclist dataset. In 2016, Xiaofei Li and team created the dataset for cyclist detection alone. The dataset contains annotated cyclists, pedestrians, motorcyclists and other riders. The training dataset contains only annotated cyclists. The validation and test dataset contains other Vulnerable Road users [VRUs], to make sure there are no false positives. The dataset also has information about occlusions, which can be used during evaluation to see how the model performs on different levels of difficult images. 

The dataset has is available publicly. This dataset is already divided into 3 parts. Train, Valid and Test. Also, there are 1000 non-VRUs available. These images do not have any objects of interest and can be used for hard negative mining.\\
Train: \\
Only cyclists which are fully visible (occlusion<10\%) and higher than 60 pixels have been labeled here.\\
Valid: \\
1019 images to be used for validation of hyper parameters. Annotations for ("pedestrian", "cyclist", "motorcyclist", "tricyclist", "wheelchairuser", "mopedrider"). Only objects higher than 20 pixels have been labeled here.\\
Test: \\
2914 images normally used for testing with annotations for ("pedestrian", "cyclist", "motorcyclist", "tricyclist", "wheelchairuser", "mopedrider"). Only objects higher than 20 pixels have been labeled here.

87.91\% of the cyclists are fully visible, 9.21\% of the cyclists are partially occluded and 2.88\% of the cyclists are heavily occluded. Only 1.23\% of the cyclists are partly truncated by the image borders.

** Ask Neeru or Anjali **

Table explaining the dataset.

\subsection{Pre-processing the dataset:}

Since, there was no data available to determine the orientation of the cyclist with respect to car, I divided the dataset into 3 separate views based on the aspect ratio of the bounding box of the cyclists. \\
1. Wide \\
2. Intermediate \\
3. Narrow \\

\[Aspect ratio =  height/width\]

If ratio < 1.1: 'wide'\\
If ratio 1.1 <= ratio < 2.1:  'intermediate'\\
If ratio 2.1 <= ratio: 'narrow'\\

After separating the instances into 3 separate classes, I had 

Narrow cyclists ~ 17000\\
Wide cyclists ~ 800\\
Intermediate ~ 1200\\

Ignore some of the data from Narrow class.
Take screen shot of wide and intermediate classes and feed them to the network.
Random horizontal flip.
Random adjusting brightness.

Last accuracy : 48 %
Initial accuracy:
3 seperate views:
Narrow = 30 %
Intermediate = 10%
Wide = 8%









\section{Accuracy}

\begin{center}
 \begin{tabular}{||c c c c c||} 
 \hline
       & Set 1 & Set 2 & Intermediate & Wide\\ [0.5ex] 
 \hline
FasterRCNN +Resnet & No finetuning & 53.2 & 22 & 35\\ 
 \hline
FasterRCNN +Resnet & Increased region proposals & 55 & 33 & 37\\
 \hline
FasterRCNN + Resnet & dropout & 59 & 32 & 39\\
 \hline
 R-FCN + Resnet & No finetuning & 50 & 33 & 23\\
 \hline

 \hline
\end{tabular}
\end{center}

\subsection{Detection}


\section{Accuracy}

\begin{center}
 \begin{tabular}{||c c c c c||} 
 \hline
 Model & Modifications & Narrow & Intermediate & Wide\\ [0.5ex] 
 \hline
FasterRCNN +Resnet & No finetuning & 53.2 & 22 & 35\\ 
 \hline
FasterRCNN +Resnet & Increased region proposals & 55 & 33 & 37\\
 \hline
FasterRCNN + Resnet & dropout & 59 & 32 & 39\\
 \hline
 R-FCN + Resnet & No finetuning & 50 & 33 & 23\\
 \hline

 \hline
\end{tabular}
\end{center}


\section{Experiment}
\subsection{Machine Specification:}
The common methods followed during any traffic scene detections are,
\begin{enumerate}
  \item Preprocessing the images
  \item Extracting region of interest
  \item Object classification
  \item Verification
\end{enumerate}


\subsubsection{1.Preprocessing of images}
sdfjksdf

\subsubsection{2.Extracting region of interest}
sdjdklsfm

\subsubsection{3.Object classification}
sdksdflkdsf

\subsubsection{4.Verification}
dsfkdslf;.

\subsection{}


\section{Conclusion}
Faster R-CNN are the mostly in fame for high accuracy. However, in case of cyclist detection a custom Region proposal system seems necessary and hence using a Fast-RCNN model with external region proposal network would be satisfactory.




\section{Future work:}

We could calculate the distance between car and cyclists to determine if any action must be taken. We could also conduct the same experiments mentioned above in dim light and night time environments to understand how well the models would perform under challenging circumstances.
\clearpage

\printbibliography

\end{document}



