
@inproceedings{xiaofeiliNewBenchmarkVisionbased2016,
  title = {A New Benchmark for Vision-Based Cyclist Detection},
  isbn = {978-1-5090-1821-5},
  url = {http://ieeexplore.ieee.org/document/7535515/},
  doi = {10.1109/IVS.2016.7535515},
  publisher = {{IEEE}},
  urldate = {2018-05-30},
  date = {2016-06},
  pages = {1028-1033},
  author = {{Xiaofei Li} and Flohr, Fabian and {Yue Yang} and {Hui Xiong} and Braun, Markus and Pan, Shuyue and {Keqiang Li} and Gavrila, Dariu M.}
}

@article{liUnifiedFrameworkConcurrent2017,
  title = {A {{Unified Framework}} for {{Concurrent Pedestrian}} and {{Cyclist Detection}}},
  volume = {18},
  issn = {1524-9050, 1558-0016},
  url = {http://ieeexplore.ieee.org/document/7506112/},
  doi = {10.1109/TITS.2016.2567418},
  number = {2},
  journaltitle = {IEEE Transactions on Intelligent Transportation Systems},
  urldate = {2018-05-30},
  date = {2017-02},
  pages = {269-281},
  author = {Li, Xiaofei and Li, Lingxi and Flohr, Fabian and Wang, Jianqiang and Xiong, Hui and Bernhard, Morys and Pan, Shuyue and Gavrila, Dariu M. and Li, Keqiang}
}

@article{rapantzikosDenseSaliencybasedSpatiotemporal,
  langid = {english},
  title = {Dense Saliency-Based Spatiotemporal Feature Points for Action Recognition},
  abstract = {Several spatiotemporal feature point detectors have been recently used in video analysis for action recognition. Feature points are detected using a number of measures, namely saliency, cornerness, periodicity, motion activity etc. Each of these measures is usually intensity-based and provides a different trade-off between density and informativeness. In this paper, we use saliency for feature point detection in videos and incorporate color and motion apart from intensity. Our method uses a multi-scale volumetric representation of the video and involves spatiotemporal operations at the voxel level. Saliency is computed by a global minimization process constrained by pure volumetric constraints, each of them being related to an informative visual aspect, namely spatial proximity, scale and feature similarity (intensity, color, motion). Points are selected as the extrema of the saliency response and prove to balance well between density and informativeness. We provide an intuitive view of the detected points and visual comparisons against state-of-the-art space-time detectors. Our detector outperforms them on the KTH dataset using NearestNeighbor classiﬁers and ranks among the top using different classiﬁcation frameworks. Statistics and comparisons are also performed on the more difﬁcult Hollywood Human Actions (HOHA) dataset increasing the performance compared to current published results.},
  pages = {8},
  author = {Rapantzikos, Konstantinos and Avrithis, Yannis and Kollias, Stefanos},
  file = {/users/mscdsa2018/dsj1/Documents/Rapantzikos et al. - Dense saliency-based spatiotemporal feature points.pdf}
}

@article{liUnifiedFrameworkConcurrent2017a,
  langid = {english},
  title = {A {{Unified Framework}} for {{Concurrent Pedestrian}} and {{Cyclist Detection}}},
  volume = {18},
  issn = {1524-9050, 1558-0016},
  url = {http://ieeexplore.ieee.org/document/7506112/},
  doi = {10.1109/TITS.2016.2567418},
  abstract = {Extensive research interest has been focused on protecting vulnerable road users in recent years, particularly pedestrians and cyclists, due to their attributes of vulnerability. However, comparatively little effort has been spent on detecting pedestrian and cyclist together, particularly when it concerns quantitative performance analysis on large datasets. In this paper, we present a uniﬁed framework for concurrent pedestrian and cyclist detection, which includes a novel detection proposal method (termed UB-MPR) to output a set of object candidates, a discriminative deep model based on Fast R-CNN for classiﬁcation and localization, and a speciﬁc postprocessing step to further improve detection performance. Experiments are performed on a new pedestrian and cyclist dataset containing 30 490 annotated pedestrian and 26 771 cyclist instances in over 50 000 images, recorded from a moving vehicle in the urban trafﬁc of Beijing. Experimental results indicate that the proposed method outperforms other state-of-the-art methods signiﬁcantly.},
  number = {2},
  journaltitle = {IEEE Transactions on Intelligent Transportation Systems},
  urldate = {2018-05-31},
  date = {2017-02},
  pages = {269-281},
  author = {Li, Xiaofei and Li, Lingxi and Flohr, Fabian and Wang, Jianqiang and Xiong, Hui and Bernhard, Morys and Pan, Shuyue and Gavrila, Dariu M. and Li, Keqiang},
  file = {/users/mscdsa2018/dsj1/Documents/Li et al. - 2017 - A Unified Framework for Concurrent Pedestrian and .pdf}
}

@article{shahrakiCyclistDetectionTracking,
  langid = {english},
  title = {Cyclist {{Detection}}, {{Tracking}}, and {{Trajectory Analysis}} in {{Urban Traffic Video Data}}},
  pages = {95},
  author = {Shahraki, Farideh Foroozandeh},
  file = {/users/mscdsa2018/dsj1/Documents/Shahraki - Cyclist Detection, Tracking, and Trajectory Analys.pdf}
}

@article{liuAUTOMATICPEDESTRIANCROSSING2017,
  langid = {english},
  title = {{{AUTOMATIC PEDESTRIAN CROSSING DETECTION AND IMPAIRMENT ANALYSIS BASED ON MOBILE MAPPING SYSTEM}}},
  volume = {IV-2/W4},
  issn = {2194-9050},
  url = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-2-W4/251/2017/},
  doi = {10.5194/isprs-annals-IV-2-W4-251-2017},
  abstract = {Pedestrian crossing, as an important part of transportation infrastructures, serves to secure pedestrians’ lives and possessions and keep traffic flow in order. As a prominent feature in the street scene, detection of pedestrian crossing contributes to 3D road marking reconstruction and diminishing the adverse impact of outliers in 3D street scene reconstruction. Since pedestrian crossing is subject to wearing and tearing from heavy traffic flow, it is of great imperative to monitor its status quo. On this account, an approach of automatic pedestrian crossing detection using images from vehicle-based Mobile Mapping System is put forward and its defilement and impairment are analyzed in this paper. Firstly, pedestrian crossing classifier is trained with low recall rate. Then initial detections are refined by utilizing projection filtering, contour information analysis, and monocular vision. Finally, a pedestrian crossing detection and analysis system with high recall rate, precision and robustness will be achieved. This system works for pedestrian crossing detection under different situations and light conditions. It can recognize defiled and impaired crossings automatically in the meanwhile, which facilitates monitoring and maintenance of traffic facilities, so as to reduce potential traffic safety problems and secure lives and property.},
  journaltitle = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
  urldate = {2018-05-31},
  date = {2017-09-13},
  pages = {251-258},
  author = {Liu, X. and Zhang, Y. and Li, Q.},
  file = {/users/mscdsa2018/dsj1/Documents/Liu et al. - 2017 - AUTOMATIC PEDESTRIAN CROSSING DETECTION AND IMPAIR.pdf}
}

@article{huFastDetectionMultiple,
  langid = {english},
  title = {Fast Detection of Multiple Objects in Trafﬁc Scenes with a Common Detection Framework},
  abstract = {Trafﬁc scene perception (TSP) aims to real-time extract accurate on-road environment information, which involves three phases: detection of objects of interest, recognition of detected objects, and tracking of objects in motion. Since recognition and tracking often rely on the results from detection, the ability to detect objects of interest effectively plays a crucial role in TSP. In this paper, we focus on three important classes of objects: trafﬁc signs, cars, and cyclists. We propose to detect all the three important objects in a single learning based detection framework. The proposed framework consists of a dense feature extractor and detectors of three important classes. Once the dense features have been extracted, these features are shared with all detectors. The advantage of using one common framework is that the detection speed is much faster, since all dense features need only to be evaluated once in the testing phase. In contrast, most previous works have designed speciﬁc detectors using different features for each of these objects. To enhance the feature robustness to noises and image deformations, we introduce spatially pooled features as a part of aggregated channel features. In order to further improve the generalization performance, we propose an object subcategorization method as a means of capturing intra-class variation of objects.},
  pages = {13},
  author = {Hu, Qichang and Paisitkriangkrai, Sakrapee and Shen, Chunhua},
  file = {/users/mscdsa2018/dsj1/Documents/Hu et al. - Fast detection of multiple objects in trafﬁc scene.pdf}
}

@article{hariyonoDetectionPedestrianCrossing2017,
  langid = {english},
  title = {Detection of Pedestrian Crossing Road: {{A}} Study on Pedestrian Pose Recognition},
  volume = {234},
  issn = {09252312},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231216315788},
  doi = {10.1016/j.neucom.2016.12.050},
  shorttitle = {Detection of Pedestrian Crossing Road},
  abstract = {Detection of pedestrian crossing road is the objective of this work. The model incorporates the pedestrian pose recognition and lateral speed, motion direction and spatial layout of the environment. Pedestrian poses are recognized according to the spatial body language ratio. The center of mass of the body relative to its width and height is used to deﬁne the pedestrian pose. Motion trajectory is obtained by using point tracking on the centroid of detected human region, and then estimated velocity is determined. Spatial layout is determined by the distance of the pedestrian to the road lane boundary. These models will be then hierarchically separated according to their action (walking, starting, bending and stopping). In order to classify the pedestrian crossing road, a walking human model is proposed. A walking human is deﬁned by ratio of the centroid location from the ground plane divided by the height of bounding box that should satisfy a constraint. The proposed algorithms are evaluated using publicly available datasets and our pedestrian walking dataset. The performance result shows that the correct pedestrian crossing road classiﬁcation is 98.10\%.},
  journaltitle = {Neurocomputing},
  urldate = {2018-05-31},
  date = {2017-04},
  pages = {144-153},
  author = {Hariyono, Joko and Jo, Kang-Hyun},
  file = {/users/mscdsa2018/dsj1/Documents/Hariyono and Jo - 2017 - Detection of pedestrian crossing road A study on .pdf}
}

@article{violaDetectingPedestriansUsing2003,
  langid = {english},
  title = {Detecting {{Pedestrians Using Patterns}} of {{Motion}} and {{Appearance}}},
  abstract = {This paper describes a pedestrian detection system that integrates image intensity information with motion information. We use a detection style algorithm that scans a detector over two consecutive frames of a video sequence. The detector is trained (using AdaBoost) to take advantage of both motion and appearance information to detect a walking person. Past approaches have built detectors based on motion information or detectors based on appearance information, but ours is the ﬁrst to combine both sources of information in a single detector. The implementation described runs at about 4 frames/second, detects pedestrians at very small scales (as small as 20x15 pixels), and has a very low false positive rate.},
  date = {2003},
  pages = {8},
  author = {Viola, Paul and Jones, Michael J and Snow, Daniel},
  file = {/users/mscdsa2018/dsj1/Documents/Viola et al. - 2003 - Detecting Pedestrians Using Patterns of Motion and.pdf}
}

@article{violaDetectingPedestriansUsing2003a,
  langid = {english},
  title = {Detecting {{Pedestrians Using Patterns}} of {{Motion}} and {{Appearance}}},
  abstract = {This paper describes a pedestrian detection system that integrates image intensity information with motion information. We use a detection style algorithm that scans a detector over two consecutive frames of a video sequence. The detector is trained (using AdaBoost) to take advantage of both motion and appearance information to detect a walking person. Past approaches have built detectors based on motion information or detectors based on appearance information, but ours is the ﬁrst to combine both sources of information in a single detector. The implementation described runs at about 4 frames/second, detects pedestrians at very small scales (as small as 20x15 pixels), and has a very low false positive rate.},
  date = {2003},
  pages = {8},
  author = {Viola, Paul and Jones, Michael J and Snow, Daniel},
  file = {/users/mscdsa2018/dsj1/Documents/Viola et al. - 2003 - Detecting Pedestrians Using Patterns of Motion and.pdf}
}

@inproceedings{salehCyclistDetectionLIDAR2017,
  langid = {english},
  title = {Cyclist Detection in {{LIDAR}} Scans Using Faster {{R}}-{{CNN}} and Synthetic Depth Images},
  isbn = {978-1-5386-1526-3},
  url = {http://ieeexplore.ieee.org/document/8317599/},
  doi = {10.1109/ITSC.2017.8317599},
  abstract = {In this paper a vision-based system for cyclist detection for highly automated vehicles is presented which utilize a deep convolutional neural network. Given the limited amount of LiDAR data for cyclists for the task of cyclist detection, a realistic synthetic data generation pipeline has been adopted in this work to overcome this problem by generate virtually unlimited number of synthetic training, testing and validation depth image datasets of cyclists. Then, using a holistic detection technique, any instances of cyclists can be localised in the generated synthetic depth images which can be shown as a down-sampled instances of 3D LiDAR data. The proposed technique in this paper, exploits the Faster Region-based Convolutional Network (Faster R-CNN) architecture for simultaneously detecting and localising any instances of cyclists in depth images. The trained Faster R-CNN models have been tested on the widely popular KITTI urban dataset and the results show that the proposed framework outperforms the classical HOG+SVM object localisation method with increased 21\% in average precision. This shows the ability of the trained models using the proposed framework to generalise from synthetic training dataset to operate on live dataset.},
  publisher = {{IEEE}},
  urldate = {2018-08-08},
  date = {2017-10},
  pages = {1-6},
  author = {Saleh, Khaled and Hossny, Mohammed and Hossny, Ahmed and Nahavandi, Saeid}
}

@inproceedings{xiaofeiliNewBenchmarkVisionbased2016a,
  langid = {english},
  title = {A New Benchmark for Vision-Based Cyclist Detection},
  isbn = {978-1-5090-1821-5},
  url = {http://ieeexplore.ieee.org/document/7535515/},
  doi = {10.1109/IVS.2016.7535515},
  abstract = {Signiﬁcant progress has been achieved over the past decade on vision-based pedestrian detection; this has led to active pedestrian safety systems being deployed in most midto high-range cars on the market. Comparatively little effort has been spent on vision-based cyclist detection, especially when it concerns quantitative performance analysis on large datasets. We present a large-scale experimental study on cyclist detection where we examine the currently most promising object detection methods; we consider Aggregated Channel Features, Deformable Part Models and Region-based Convolutional Neural Networks. We also introduce a new method called Stereo-Proposal based Fast R-CNN (SP-FRCN) to detect cyclists based on stereo proposals and Fast R-CNN (FRCN) framework. Experiments are performed on a dataset containing 22161 annotated cyclist instances in over 30000 images, recorded from a moving vehicle in the urban trafﬁc of Beijing. Results indicate that all the three solution families can reach top performance around 0.89 average precision on the easy case, but the performance drops gradually with the difﬁculty increasing. The dataset including rich annotations, stereo images and evaluation scripts (termed “Tsinghua-Daimler Cyclist Benchmark”) is made public to the scientiﬁc community, to serve as a common point of reference for future research.},
  publisher = {{IEEE}},
  urldate = {2018-08-08},
  date = {2016-06},
  pages = {1028-1033},
  author = {{Xiaofei Li} and Flohr, Fabian and {Yue Yang} and {Hui Xiong} and Braun, Markus and Pan, Shuyue and {Keqiang Li} and Gavrila, Dariu M.},
  file = {/users/mscdsa2018/dsj1/Downloads/Xiaofei Li et al. - 2016 - A new benchmark for vision-based cyclist detection.pdf}
}

@article{janaiComputerVisionAutonomous2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.05519},
  primaryClass = {cs},
  langid = {english},
  title = {Computer {{Vision}} for {{Autonomous Vehicles}}: {{Problems}}, {{Datasets}} and {{State}}-of-the-{{Art}}},
  url = {http://arxiv.org/abs/1704.05519},
  shorttitle = {Computer {{Vision}} for {{Autonomous Vehicles}}},
  abstract = {Recent years have witnessed amazing progress in AI related ﬁelds such as computer vision, machine learning and autonomous vehicles. As with any rapidly growing ﬁeld, however, it becomes increasingly diﬃcult to stay up-to-date or enter the ﬁeld as a beginner. While several topic speciﬁc survey papers have been written, to date no general survey on problems, datasets and methods in computer vision for autonomous vehicles exists. This paper attempts to narrow this gap by providing a state-of-the-art survey on this topic. Our survey includes both the historically most relevant literature as well as the current state-of-the-art on several speciﬁc topics, including recognition, reconstruction, motion estimation, tracking, scene understanding and end-to-end learning. Towards this goal, we ﬁrst provide a taxonomy to classify each approach and then analyze the performance of the state-of-the-art on several challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we will also provide an interactive platform which allows to navigate topics and methods, and provides additional information and project links for each paper.},
  urldate = {2018-08-08},
  date = {2017-04-18},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  author = {Janai, Joel and Güney, Fatma and Behl, Aseem and Geiger, Andreas},
  file = {/users/mscdsa2018/dsj1/Downloads/Janai et al. - 2017 - Computer Vision for Autonomous Vehicles Problems,.pdf}
}

@article{daiRFCNObjectDetection2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.06409},
  primaryClass = {cs},
  langid = {english},
  title = {R-{{FCN}}: {{Object Detection}} via {{Region}}-Based {{Fully Convolutional Networks}}},
  url = {http://arxiv.org/abs/1605.06409},
  shorttitle = {R-{{FCN}}},
  abstract = {We present region-based, fully convolutional networks for accurate and efﬁcient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN [6, 18] that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classiﬁcation and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classiﬁer backbones, such as the latest Residual Networks (ResNets) [9], for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6\% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20× faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn.},
  urldate = {2018-08-08},
  date = {2016-05-20},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
  file = {/users/mscdsa2018/dsj1/Downloads/Dai et al. - 2016 - R-FCN Object Detection via Region-based Fully Con.pdf}
}

@article{renFasterRCNNRealTime2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.01497},
  primaryClass = {cs},
  langid = {english},
  title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
  url = {http://arxiv.org/abs/1506.01497},
  shorttitle = {Faster {{R}}-{{CNN}}},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with “attention” mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  urldate = {2018-08-08},
  date = {2015-06-04},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  file = {/users/mscdsa2018/dsj1/Downloads/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection w.pdf}
}

@article{suhaoVehicleTypeDetection2018,
  langid = {english},
  title = {Vehicle Type Detection Based on Deep Learning in Traffic Scene},
  volume = {131},
  issn = {18770509},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S1877050918306616},
  doi = {10.1016/j.procs.2018.04.281},
  journaltitle = {Procedia Computer Science},
  urldate = {2018-08-08},
  date = {2018},
  pages = {564-572},
  author = {Suhao, Li and Jinzhao, Lin and Guoquan, Li and Tong, Bai and Huiqian, Wang and Yu, Pang},
  file = {/users/mscdsa2018/dsj1/Downloads/Suhao et al. - 2018 - Vehicle type detection based on deep learning in t.pdf}
}

@online{MultilevelMixtureofExpertsFramework,
  title = {A {{Multilevel Mixture}}-of-{{Experts Framework}} for {{Pedestrian Classification}} - {{IEEE Journals}} \& {{Magazine}}},
  url = {https://ieeexplore.ieee.org/document/5749283/?part=1},
  urldate = {2018-08-08},
  file = {/users/mscdsa2018/dsj1/Zotero/storage/MZTDACZZ/5749283.html}
}

@inproceedings{tianFastCyclistDetection2015,
  title = {Fast {{Cyclist Detection}} by {{Cascaded Detector}} and {{Geometric Constraint}}},
  doi = {10.1109/ITSC.2015.211},
  abstract = {In this paper we present a vision-based detection system for cyclists. We build cascaded detectors with different classifiers and shared features to detect cyclists from multiple viewpoints. To improve the performance, we reveal the dependence between the size and the position of an object in the image by a regression method. We also explore the applications of this geometric constraint with different camera setups. Based on experiments we demonstrate that our detector is suitable for real time applications.},
  eventtitle = {2015 {{IEEE}} 18th {{International Conference}} on {{Intelligent Transportation Systems}}},
  booktitle = {2015 {{IEEE}} 18th {{International Conference}} on {{Intelligent Transportation Systems}}},
  date = {2015-09},
  pages = {1286-1291},
  keywords = {bicycles,Bicycles,camera setup,Cameras,cascaded detector,classification,computer vision,Detectors,driver assistance system,driver information systems,fast cyclist detection,feature extraction,Feature extraction,feature sharing,geometric constraint,image classification,object detection,object position,object size,Real-time systems,regression analysis,regression method,road safety,Support vector machines,Training,vision-based detection system},
  author = {Tian, W. and Lauer, M.},
  file = {/users/mscdsa2018/dsj1/Zotero/storage/LGS5W68X/7313303.html}
}

@article{enzweilerMultilevelMixtureofExpertsFramework2011,
  title = {A {{Multilevel Mixture}}-of-{{Experts Framework}} for {{Pedestrian Classification}}},
  volume = {20},
  issn = {1057-7149},
  doi = {10.1109/TIP.2011.2142006},
  abstract = {Notwithstanding many years of progress, pedestrian recognition is still a difficult but important problem. We present a novel multilevel Mixture-of-Experts approach to combine information from multiple features and cues with the objective of improved pedestrian classification. On pose-level, shape cues based on Chamfer shape matching provide sample-dependent priors for a certain pedestrian view. On modality-level, we represent each data sample in terms of image intensity, (dense) depth, and (dense) flow. On feature-level, we consider histograms of oriented gradients (HOG) and local binary patterns (LBP). Multilayer perceptrons (MLP) and linear support vector machines (linSVM) are used as expert classifiers. Experiments are performed on a unique real-world multi-modality dataset captured from a moving vehicle in urban traffic. This dataset has been made public for research purposes. Our results show a significant performance boost of up to a factor of 42 in reduction of false positives at constant detection rates of our approach compared to a baseline intensity-only HOG/linSVM approach.},
  number = {10},
  journaltitle = {IEEE Transactions on Image Processing},
  date = {2011-10},
  pages = {2967-2979},
  keywords = {Cameras,Chamfer shape matching,Cluster Analysis,Computational modeling,depth,flow,Fuzzy Logic,histograms of oriented gradients,HOG,Humans,image classification,image intensity,image matching,Image Processing; Computer-Assisted,LBP,linear support vector machines,linSVM,local binary patterns,Mixture-of-experts,MLP,multilayer perceptrons,multilevel mixture-of-experts framework,multimodality dataset,object detection,Optical imaging,Pattern Recognition; Automated,pedestrian classification,pedestrian recognition,Pixel,Shape,shape recognition,support vector machines,Support vector machines,Support Vector Machines,traffic engineering computing,Training},
  author = {Enzweiler, M. and Gavrila, D. M.},
  file = {/users/mscdsa2018/dsj1/Zotero/storage/YAMY6CTS/Enzweiler and Gavrila - 2011 - A Multilevel Mixture-of-Experts Framework for Pede.pdf;/users/mscdsa2018/dsj1/Zotero/storage/HJGVSPAB/5749283.html}
}

@article{chen3DObjectProposals2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.07711},
  primaryClass = {cs},
  title = {{{3D Object Proposals}} Using {{Stereo Imagery}} for {{Accurate Object Class Detection}}},
  url = {http://arxiv.org/abs/1608.07711},
  abstract = {The goal of this paper is to perform 3D object detection in the context of autonomous driving. Our method first aims at generating a set of high-quality 3D object proposals by exploiting stereo imagery. We formulate the problem as minimizing an energy function that encodes object size priors, placement of objects on the ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground. We then exploit a CNN on top of these proposals to perform object detection. In particular, we employ a convolutional neural net (CNN) that exploits context and depth information to jointly regress to 3D bounding box coordinates and object pose. Our experiments show significant performance gains over existing RGB and RGB-D object proposal methods on the challenging KITTI benchmark. When combined with the CNN, our approach outperforms all existing results in object detection and orientation estimation tasks for all three KITTI object classes. Furthermore, we experiment also with the setting where LIDAR information is available, and show that using both LIDAR and stereo leads to the best result.},
  urldate = {2018-08-19},
  date = {2016-08-27},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Chen, Xiaozhi and Kundu, Kaustav and Zhu, Yukun and Ma, Huimin and Fidler, Sanja and Urtasun, Raquel},
  file = {/users/mscdsa2018/dsj1/Zotero/storage/YDWZ8EFV/Chen et al. - 2016 - 3D Object Proposals using Stereo Imagery for Accur.pdf;/users/mscdsa2018/dsj1/Zotero/storage/WLT4QWSM/1608.html}
}

@online{MultipleObjectsFusion,
  title = {Multiple {{Objects Fusion Tracker Using}} a {{Matching Network}} for {{Adaptively Represented Instance Pairs}}},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5424760/},
  urldate = {2018-08-19},
  file = {/users/mscdsa2018/dsj1/Zotero/storage/4G2Q33U5/PMC5424760.html}
}

@article{ohMultipleObjectsFusion2017,
  title = {Multiple {{Objects Fusion Tracker Using}} a {{Matching Network}} for {{Adaptively Represented Instance Pairs}}},
  volume = {17},
  issn = {1424-8220},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5424760/},
  doi = {10.3390/s17040883},
  abstract = {Multiple-object tracking is affected by various sources of distortion, such as occlusion, illumination variations and motion changes. Overcoming these distortions by tracking on RGB frames, such as shifting, has limitations because of material distortions caused by RGB frames. To overcome these distortions, we propose a multiple-object fusion tracker (MOFT), which uses a combination of 3D point clouds and corresponding RGB frames. The MOFT uses a matching function initialized on large-scale external sequences to determine which candidates in the current frame match with the target object in the previous frame. After conducting tracking on a few frames, the initialized matching function is fine-tuned according to the appearance models of target objects. The fine-tuning process of the matching function is constructed as a structured form with diverse matching function branches. In general multiple object tracking situations, scale variations for a scene occur depending on the distance between the target objects and the sensors. If the target objects in various scales are equally represented with the same strategy, information losses will occur for any representation of the target objects. In this paper, the output map of the convolutional layer obtained from a pre-trained convolutional neural network is used to adaptively represent instances without information loss. In addition, MOFT fuses the tracking results obtained from each modality at the decision level to compensate the tracking failures of each modality using basic belief assignment, rather than fusing modalities by selectively using the features of each modality. Experimental results indicate that the proposed tracker provides state-of-the-art performance considering multiple objects tracking (MOT) and KITTIbenchmarks.},
  number = {4},
  journaltitle = {Sensors (Basel, Switzerland)},
  shortjournal = {Sensors (Basel)},
  urldate = {2018-08-19},
  date = {2017-04-18},
  author = {Oh, Sang-Il and Kang, Hang-Bong},
  file = {/users/mscdsa2018/dsj1/Zotero/storage/BKKYWUPQ/Oh and Kang - 2017 - Multiple Objects Fusion Tracker Using a Matching N.pdf},
  eprinttype = {pmid},
  eprint = {28420194},
  pmcid = {PMC5424760}
}

@inproceedings{dalalHistogramsOrientedGradients2005,
  title = {Histograms of Oriented Gradients for Human Detection},
  volume = {1},
  doi = {10.1109/CVPR.2005.177},
  abstract = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
  eventtitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  date = {2005-06},
  pages = {886-893 vol. 1},
  keywords = {coarse spatial binning,contrast normalization,edge based descriptors,feature extraction,fine orientation binning,fine-scale gradients,gradient based descriptors,gradient methods,High performance computing,Histograms,histograms of oriented gradients,human detection,Humans,Image databases,Image edge detection,linear SVM,object detection,Object detection,object recognition,Object recognition,overlapping descriptor,pedestrian database,robust visual object recognition,Robustness,support vector machines,Support vector machines,Testing},
  author = {Dalal, N. and Triggs, B.},
  file = {/users/mscdsa2018/dsj1/Zotero/storage/QPN3SG85/Dalal and Triggs - 2005 - Histograms of oriented gradients for human detecti.pdf;/users/mscdsa2018/dsj1/Zotero/storage/DAY3ZA94/1467360.html}
}

@article{felzenszwalbObjectDetectionDiscriminatively2010,
  title = {Object {{Detection}} with {{Discriminatively Trained Part}}-{{Based Models}}},
  volume = {32},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2009.167},
  abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI–SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.},
  number = {9},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  date = {2010-09},
  pages = {1627-1645},
  keywords = {Algorithms,Artificial Intelligence,Bicycles,Computer Society,Computer vision,data mining,deformable models,Deformable models,Discriminant Analysis,discriminative trained part-based models,discriminative training,Image Enhancement,Image Interpretation; Computer-Assisted,Imaging; Three-Dimensional,Iterative algorithms,iterative methods,iterative training algorithm,latent SVM objective function,latent SVM.,Lighting,margin-sensitive approach,multiscale deformable part models,object detection,Object detection,object detection system,object recognition,Object recognition,PASCAL object detection,Pattern Recognition; Automated,pictorial structures,Reproducibility of Results,Sensitivity and Specificity,Shape,Speech recognition,support vector machine,support vector machines,Support vector machines},
  author = {Felzenszwalb, P. F. and Girshick, R. B. and McAllester, D. and Ramanan, D.},
  file = {/users/mscdsa2018/dsj1/Zotero/storage/C2KLFZ43/Felzenszwalb et al. - 2010 - Object Detection with Discriminatively Trained Par.pdf;/users/mscdsa2018/dsj1/Zotero/storage/P45PSUB5/5255236.html}
}

@inproceedings{sujanaRealTimeObject2017,
  title = {Real Time Object Identification Using Deep Convolutional Neural Networks},
  doi = {10.1109/ICCSP.2017.8286705},
  abstract = {The project on Real Time Object Identification presents an approach to use the concept of deep learning with the convolutional neural networks in identifying the objects present when video is given as the input. The method uses the input video to give the output with the set of identified objects surrounded by boxes, even if the objects are of different sizes and shapes. Along with identification of the objects, the convolutional neural network works give the confidence score for each of the object. This methodology is called the Single Shot MultiBox Detector (SSD). In this project, we are replacing the VGG Net with Residual Networks in the architecture to increase the computational speed.},
  eventtitle = {2017 {{International Conference}} on {{Communication}} and {{Signal Processing}} ({{ICCSP}})},
  booktitle = {2017 {{International Conference}} on {{Communication}} and {{Signal Processing}} ({{ICCSP}})},
  date = {2017-04},
  pages = {1801-1805},
  keywords = {Computer architecture,convolution,convolutional neural network works,Convolutional neural networks,deep convolutional neural networks,deep learning,Detectors,learning (artificial intelligence),Microsoft Windows,neural nets,object recognition,Object recognition,real-time object identification,Real-time object identification,Real-time systems,Residual Networks,Single Shot Multibox Detector,single-shot multibox detector,SSD,Training,VGGNet,video signal processing},
  author = {Sujana, S. R. and Abisheck, S. S. and Ahmed, A. T. and Chandran, K. R. S.},
  file = {/users/mscdsa2018/dsj1/Zotero/storage/8JWW3MXY/Sujana et al. - 2017 - Real time object identification using deep convolu.pdf;/users/mscdsa2018/dsj1/Zotero/storage/W6LVUCAV/8286705.html}
}

@incollection{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2018-08-19},
  date = {2012},
  pages = {1097--1105},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  file = {/users/mscdsa2018/dsj1/Zotero/storage/JXIINN2W/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf;/users/mscdsa2018/dsj1/Zotero/storage/W9WI9S38/4824-imagenet-classification-with-deep-convolutional-neural-networks.html}
}


